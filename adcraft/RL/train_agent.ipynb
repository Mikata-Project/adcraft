{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook gives an example of how to train A2C, TD3, and PPO agents on AdCraft environment using the agent configurations provided in experiment_utils/experiment_configs.py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from adcraft.experiment_utils import experiment_configs\n",
    "import adcraft.gymnasium_kw_env as kw_sim\n",
    "from adcraft.experiment_utils.experiment_quantiles import (\n",
    "    make_experiment_quantiles, load_experiment_quantiles)\n",
    "from adcraft.experiment_utils.experiment_metrics import (\n",
    "    get_implicit_kw_bid_cpc_impressions, get_max_expected_bid_profits, compute_AKNCP, compute_NCP)\n",
    "from adcraft.wrappers.flat_array import FlatArrayWrapper\n",
    "from adcraft.experiment_utils.experiment_configs import (dense_env_config, semi_dense_env_config, \n",
    "very_sparse_env_config, sparse_env_config, non_stationary_sparse_env_config, \n",
    "non_stationary_dense_env_config)\n",
    "\n",
    "from ray.tune.registry import register_env\n",
    "from ray.rllib.algorithms.algorithm import Algorithm\n",
    "from ray.rllib.policy.sample_batch import SampleBatch\n",
    "from ray.rllib.policy.torch_policy_template import build_torch_policy\n",
    "from ray.rllib.utils.torch_utils import apply_grad_clipping, sequence_mask\n",
    "from ray.rllib.evaluation.episode import Episode\n",
    "from ray.rllib.evaluation.postprocessing import (\n",
    "    compute_gae_for_sample_batch,\n",
    "    Postprocessing)\n",
    "from ray.rllib.models.modelv2 import ModelV2\n",
    "from ray.rllib.models.torch.torch_action_dist import TorchDistributionWrapper\n",
    "from ray.rllib.policy.sample_batch import SampleBatch\n",
    "from ray.rllib.policy.torch_mixins import (\n",
    "    EntropyCoeffSchedule,\n",
    "    LearningRateSchedule,\n",
    "    ValueNetworkMixin)\n",
    "from ray.rllib.utils.typing import (\n",
    "    LocalOptimizer,\n",
    "    SpaceStruct,\n",
    "    TensorStructType,\n",
    "    TensorType)\n",
    "from ray.rllib.policy.torch_policy_v2 import TorchPolicyV2\n",
    "from ray.rllib.utils.annotations import override\n",
    "from ray.rllib.utils.framework import try_import_torch\n",
    "from ray.rllib.utils.numpy import convert_to_numpy\n",
    "from ray.rllib.utils.typing import AgentID, TensorType\n",
    "from ray.rllib.algorithms.ppo import PPO, PPOConfig\n",
    "from ray.rllib.algorithms.a2c import A2C, A2CConfig\n",
    "from ray.rllib.algorithms.TD3 import TD3, TD3Config\n",
    "\n",
    "from adcraft.experiment_utils.agent_configs import sem_ppo_config\n",
    "from adcraft.experiment_utils.agent_configs import sem_a2c_config\n",
    "from adcraft.experiment_utils.agent_configs import sem_td3_config\n",
    "\n",
    "from adcraft.experiment_utils.experiment_configs import NUM_KEYWORDS\n",
    "from adcraft.experiment_utils.experiment_configs import MAX_DAYS\n",
    "from adcraft.experiment_utils.experiment_configs import experiment_mode\n",
    "\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import yaml\n",
    "from datetime import datetime\n",
    "import csv"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_KEYWORDS = NUM_KEYWORDS\n",
    "MAX_DAYS = MAX_DAYS\n",
    "experiment_type = experiment_mode\n",
    "NUM_ITERATIONS = 500\n",
    "evaluation_interval = 5\n",
    "\n",
    "model = 'A2C'\n",
    "is_from_checkpoint = False\n",
    "is_new_config = False\n",
    "new_model_config = None\n",
    "checkpoint_path = \"\"\n",
    "bound = NUM_ITERATIONS // evaluation_interval\n",
    "updater_params = [0.3]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Environment Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if experiment_type == \"semi_dense\":\n",
    "    env_config = semi_dense_env_config\n",
    "    volume = 64\n",
    "    cvr = 0.8\n",
    "    mean_volumes = [64]\n",
    "    conversion_rates = [0.8]\n",
    "elif experiment_type == \"sparse\":\n",
    "    env_config = sparse_env_config\n",
    "    volume = 64\n",
    "    cvr = 0.1\n",
    "    mean_volumes = [64]\n",
    "    conversion_rates = [0.1]\n",
    "elif experiment_type == \"very_sparse\":\n",
    "    env_config = very_sparse_env_config\n",
    "    volume = 16\n",
    "    cvr = 0.1\n",
    "    mean_volumes = [16]\n",
    "    conversion_rates = [0.1]\n",
    "elif experiment_type == \"non_stationary_dense\":\n",
    "    env_config = non_stationary_dense_env_config\n",
    "    volume = 128\n",
    "    cvr = 0.8\n",
    "    mean_volumes = [128]\n",
    "    conversion_rates = [0.8]\n",
    "elif experiment_type == \"non_stationary_sparse\":\n",
    "    env_config = non_stationary_sparse_env_config\n",
    "    volume = 64\n",
    "    cvr = 0.1\n",
    "    mean_volumes = [64]\n",
    "    conversion_rates = [0.1]\n",
    "else: \n",
    "    env_config = dense_env_config\n",
    "    volume = 128\n",
    "    cvr = 0.8\n",
    "    mean_volumes = [128]\n",
    "    conversion_rates = [0.8]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_agent_for_max_days(env, agent, irs, cpcs):\n",
    "    # obs, infos = env.reset(seed=env_seed)\n",
    "    obs, infos = env.reset()\n",
    "    action,_,_ = agent.get_policy(policy_id=\"default_policy\").compute_single_action(obs=obs, info = infos)\n",
    "    # print(env.updater_params)\n",
    "    rewards = [] \n",
    "    kw_profits = []\n",
    "    ideal_profits = []\n",
    "    for i in range(env.max_days):\n",
    "        # compute ideal profits for this timestep\n",
    "        ideal_profit = []\n",
    "        for kw_index, kw_params in enumerate(env.keyword_params):\n",
    "            max_exp_profit, positive_proportion = get_max_expected_bid_profits(\n",
    "                kw_params, cpcs[kw_index], irs[kw_index]\n",
    "            )\n",
    "            ideal_profit.append(max_exp_profit)\n",
    "        ideal_profits.append(ideal_profit)\n",
    "        previous_observation, reward,_,_,info = env.step(action)\n",
    "        rewards.append(reward)\n",
    "        ######## Due to the flattening instead of using the dict format of the observation we should explicitly \n",
    "        ######## slice the observation to retrive the cost and revenue arrays\n",
    "        previous_observation_revenue = previous_observation[3*NUM_KEYWORDS+2:4*NUM_KEYWORDS+2]\n",
    "        previous_observation_cost = previous_observation[1*NUM_KEYWORDS:2*NUM_KEYWORDS]\n",
    "        kw_profits.append(previous_observation_revenue - previous_observation_cost)\n",
    "        action,_,_ = agent.get_policy(policy_id=\"default_policy\").compute_single_action(obs=previous_observation, info = info)\n",
    "    \n",
    "    return rewards, kw_profits, ideal_profits, compute_AKNCP(np.array(kw_profits), np.array(ideal_profits)), compute_AKNCP_mean(np.array(kw_profits), np.array(ideal_profits)), compute_NCP(np.array(kw_profits), np.array(ideal_profits))\n",
    "\n",
    "def run_sparsity_experiments(model, agent, mean_volumes, conversion_rates, num_keywords=NUM_KEYWORDS, time_steps=MAX_DAYS):\n",
    "    allowed_bids = np.arange(0.01, 3.00, 0.01)\n",
    "    for volume in mean_volumes:\n",
    "        for cvr in conversion_rates:\n",
    "            env = FlatArrayWrapper(kw_sim.bidding_sim_creator(env_config))\n",
    "            # RUN EXPERIMENT ON ENV\n",
    "            results_dir = str(Path.cwd().as_posix())+\"OpenMail/ds-scratch/maziar/sem-se/experiment_results/\"+f\"{model}_{volume}_{cvr}/\"\n",
    "            if not os.path.isdir(results_dir):\n",
    "                os.mkdir(results_dir)  \n",
    "\n",
    "            eps_rewards = []  \n",
    "            eps_AKNCP = []\n",
    "            eps_AKNCP_mean = []\n",
    "            eps_NCP = []\n",
    "\n",
    "            for env_seed in range(5,10):\n",
    "                env.reset(seed=env_seed)\n",
    "                irs, cpcs = [],[]\n",
    "                for kw in env.keywords:\n",
    "                    ir, cpc = get_implicit_kw_bid_cpc_impressions(kw, allowed_bids)\n",
    "                    irs.append(ir)\n",
    "                    cpcs.append(cpc)\n",
    "                rewards, kw_profits, ideal_profits, AKNCP, AKNCP_mean, NCP = run_agent_for_max_days(env, agent, irs, cpcs)\n",
    "                # np.savez(results_dir+f\"maziar_experiment.npz\", kw_profits=kw_profits, ideal_profits=ideal_profits)\n",
    "                np.savez(results_dir+f\"{model}_{NUM_ITERATIONS}_iteration_{experiment_type}.npz\", env_seed=env_seed, kw_profits=kw_profits, ideal_profits=ideal_profits)\n",
    "                eps_rewards.append(np.mean(rewards))\n",
    "                eps_AKNCP.append(AKNCP)\n",
    "                eps_AKNCP_mean.append(AKNCP_mean)\n",
    "                eps_NCP.append(NCP)\n",
    "                print(' ' + str(env_seed), end = ' ')\n",
    "\n",
    "\n",
    "            values = [np.mean(np.array(eps_rewards)), sem(np.array(eps_rewards)), np.mean(np.array(eps_AKNCP)), sem(np.array(eps_AKNCP)) , np.mean(np.array(eps_AKNCP_mean)), sem(np.array(eps_AKNCP_mean)), np.mean(np.array(eps_NCP)), sem(np.array(eps_NCP))]\n",
    "            pile_values_to_json(values,results_dir+f\"{model}_{NUM_ITERATIONS}_iteration_{experiment_type}.json\")\n",
    "            print(f\"vol, cvr: ({volume}, {cvr}) evaluation done!\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Environment Registration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def env_creator(env_config = env_config):\n",
    "    return FlatArrayWrapper(kw_sim.bidding_sim_creator(env_config=env_config))\n",
    "\n",
    "register_env(\"FlatArrayAuction\", env_creator)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Instantiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_from_checkpoint == True:\n",
    "    agent = Algorithm.from_checkpoint(checkpoint_path)\n",
    "    agent.restore(checkpoint_path)\n",
    "    if is_new_config == True:\n",
    "        agent.reset_config(new_model_config)\n",
    "else: \n",
    "    if model == \"A2C\":\n",
    "        model_config = sem_a2c_config\n",
    "    elif model == \"TD3\":\n",
    "        model_config = sem_td3_config\n",
    "    else:\n",
    "        model_config = sem_ppo_config\n",
    "    \n",
    "    agent = model_config.build()\n",
    "\n",
    "env = kw_sim.bidding_sim_creator(env_config=env_config)\n",
    "obs, infos = env.reset()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Run Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(NUM_ITERATIONS):\n",
    "    agent.train()\n",
    "    print(f\"Iteration number {str(i)} of {model} is completed\")\n",
    "    if ((i+1) % evaluation_interval) == 0:\n",
    "        run_sparsity_experiments(model, agent, mean_volumes, conversion_rates)\n",
    "results_dir = f\"./{model}_{updater_params[0]}/model/\"\n",
    "if not os.path.isdir(results_dir):\n",
    "    os.mkdir(results_dir)\n",
    "checkpoint_path = agent.save(results_dir)\n",
    "print(\"An Algorithm checkpoint has been created inside directory: \"f\"{checkpoint_path}.\")\n",
    "agent.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datasci",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
